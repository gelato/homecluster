---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-stack
  namespace: observability
spec:
  interval: 30m
  chart:
    spec:
      chart: victoria-metrics-k8s-stack
      version: 0.30.0
      sourceRef:
        kind: HelmRepository
        name: victoriametrics-charts
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  driftDetection:
    mode: enabled
  values:
    global:
      # -- Cluster label to use for dashboards and rules
      clusterLabel: ark
      cluster:
        # -- K8s cluster domain suffix, uses for building storage pods' FQDN. Details are [here](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/)
        dnsDomain: cluster.local.

    fullnameOverride: victoria-metrics

    # -- Tenant to use for Grafana datasources and remote write
    tenant: "0"

    # VM Operator deployment
    victoria-metrics-operator:
      enabled: true
      crds:
        # -- added temporary, till new operator version released
        plain: true
        cleanup:
          enabled: true
      serviceMonitor:
        enabled: true
      operator:
        # -- By default, operator converts prometheus-operator objects.
        disable_prometheus_converter: false

    defaultDashboards:
      # -- Enable custom dashboards installation
      enabled: true
      defaultTimezone: utc+3
      labels: {}
      annotations: {}
      grafanaOperator:
        # -- Create dashboards as CRDs (reuqires grafana-operator to be installed)
        enabled: false
        spec:
          instanceSelector:
            matchLabels:
              dashboards: "grafana"
          allowCrossNamespaceImport: false
      # -- Create dashboards as ConfigMap despite dependency it requires is not installed
      dashboards:
        victoriametrics-vmalert:
          enabled: true
        victoriametrics-operator:
          enabled: true
        # -- In ArgoCD using client-side apply this dashboard reaches annotations size limit and causes k8s issues without server side apply
        # See [this issue](https://github.com/VictoriaMetrics/helm-charts/tree/disable-node-exporter-dashboard-by-default/charts/victoria-metrics-k8s-stack#metadataannotations-too-long-must-have-at-most-262144-bytes-on-dashboards)
        node-exporter-full:
          enabled: true

    # -- Create default rules for monitoring the cluster
    defaultRules:
      create: true

      groups:
        etcd:
          create: true
        general:
          create: true
        k8sContainerMemoryRss:
          create: true
        k8sContainerMemoryCache:
          create: true
        k8sContainerCpuUsageSecondsTotal:
          create: true
        k8sPodOwner:
          create: true
        k8sContainerResource:
          create: true
        k8sContainerMemoryWorkingSetBytes:
          create: true
        k8sContainerMemorySwap:
          create: true
        kubeApiserver:
          create: true
        kubeApiserverAvailability:
          create: true
        kubeApiserverBurnrate:
          create: true
        kubeApiserverHistogram:
          create: true
        kubeApiserverSlos:
          create: true
        kubelet:
          create: true
        kubePrometheusGeneral:
          create: true
        kubePrometheusNodeRecording:
          create: true
        kubernetesApps:
          create: true
          targetNamespace: ".*"
        kubernetesResources:
          create: true
        kubernetesStorage:
          create: true
          targetNamespace: ".*"
        kubernetesSystem:
          create: true
        kubernetesSystemKubelet:
          create: true
        kubernetesSystemApiserver:
          create: true
        kubernetesSystemControllerManager:
          create: true
        kubeScheduler:
          create: true
        kubernetesSystemScheduler:
          create: true
        kubeStateMetrics:
          create: true
        nodeNetwork:
          create: true
        node:
          create: true
        vmagent:
          create: true
        vmsingle:
          create: false
        vmcluster:
          create: true
        vmHealth:
          create: true
        vmoperator:
          create: true
        alertmanager:
          create: true

      # -- Runbook url prefix for default rules
      runbookUrl: https://runbooks.prometheus-operator.dev/runbooks

    # Main VM Cluster in HA mode.
    vmcluster:
      enabled: true
      spec:
        # Duration to keep metrics
        retentionPeriod: "2" # this is months unless a character prefix is added (1d, 1w, etc)
        replicationFactor: 2
        # Storage backend
        vmstorage:
          replicaCount: 2 # HA - but prefer 3 as i have 3 notes & using local-hostpath
          # required for HA vmagents
          extraArgs:
            dedup.minScrapeInterval: 60s # Only needed for HA
          resources:
            requests:
              cpu: 10m
              memory: 300Mi
            limits:
              memory: 6Gi
          topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmstorage
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: local-hostpath
                resources:
                  requests:
                    storage: 25Gi

        # Query backend
        vmselect:
          replicaCount: 2 # HA
          cacheMountPath: /select-cache
          extraArgs:
            dedup.minScrapeInterval: 60s
            vmstorageDialTimeout: 5s
            search.maxConcurrentRequests: "10"
            search.minStalenessInterval: 5m
            loggerTimezone: ${TIMEZONE}
            search.maxUniqueTimeseries: "1000000"
          resources:
            requests:
              cpu: 10m
              memory: 300Mi
            limits:
              memory: 6Gi
          topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmselect
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: longhorn-fast
                resources:
                  requests:
                    storage: 4Gi
        # Data ingestion backend
        vminsert:
          replicaCount: 2 # HA
          extraArgs:
            maxLabelsPerTimeseries: "90" # More needed if you are scraping a LOT of stuff and having issues
            # insert.maxConcurrentRequests: 10
          resources:
            requests:
              cpu: 10m
              memory: 300Mi
            limits:
              memory: 6Gi
          topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vminsert
      ingress:
        storage:
          enabled: true
          ingressClassName: internal
          hosts:
            - &hoststorage vmstorage.${SECRET_DOMAIN}
          tls:
            - hosts:
                - *hoststorage

        select:
          enabled: true
          ingressClassName: internal
          hosts:
            - &hostselect vmselect.${SECRET_DOMAIN}
          tls:
            - hosts:
                - *hostselect

        insert:
          enabled: true
          ingressClassName: internal
          hosts:
            - &hostinsert vminsert.${SECRET_DOMAIN}
          tls:
            - hosts:
                - *hostinsert

    # Prometheus alertmanager spinup
    # Wait, why are we using vmalert AND prometheus alertmanager???
    # VMalert isnt fully featured and VM acknowledge its a favourite
    # for example,
    alertmanager:
      enabled: true

      ingress:
        enabled: true
        ingressClassName: internal
        hosts:
          - &hostam alertmanager.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostam
      spec:
        selectAllByDefault: true
        externalURL: https://alertmanager.${SECRET_DOMAIN}
        replicaCount: 2
        # Import Alertmanager config from a existing secret
        # this can be configured directly in chart but it can
        #   be a bit unweildy to manage
        configSecret: alertmanager-secret
        # I needed this for talos using local-hostpath - YMMV
        #
        securityContext:
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          fsGroup: 2000
          seccompProfile:
            type: RuntimeDefault

        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-fast
              resources:
                requests:
                  storage: 50Mi

      # Extra slack templates
      monzoTemplate:
        enabled: false

    # VM Alerting (however, this just watches & passes alerts to prom-alertmanager below...)
    vmalert:
      enabled: true
      spec:
        replicaCount: 2 # HA
        extraArgs:
          external.url: https://vmalert.${SECRET_DOMAIN}
        evaluationInterval: "60s"
        resources:
          requests:
            cpu: 10m
            memory: 128Mi
          limits:
            cpu: 150m
            memory: 6Gi
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmalert
        notifiers:
          - selector:
              namespaceSelector:
                matchNames: ["observability"]
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: victoria-metrics-k8s-stack
      ingress:
        enabled: true
        ingressClassName: internal
        hosts:
          - &hostalert vmalert.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostalert

    # VM Data scraping
    vmagent:
      enabled: true
      spec:
        shardCount: 2
        selectAllByDefault: true
        extraArgs:
          promscrape.streamParse: "true"
          sortLabels: "true"
          promscrape.maxScrapeSize: "100MB"
          # Do not store original labels in vmagent's memory by default. This reduces the amount of memory used by vmagent
          # but makes vmagent debugging UI less informative. See: https://docs.victoriametrics.com/vmagent/#relabel-debug
          promscrape.dropOriginalLabels: "true"
        scrapeInterval: "30s"
        additionalScrapeConfigs:
          name: vm-additional-scrape-configs
          key: prometheus-additional.yaml
        externalLabels:
          cluster: ark
        remoteWriteSettings:
          label:
            cluster: ark
        resources:
          requests:
            cpu: 10m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmagent
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          nginx.ingress.kubernetes.io/auth-url: |-
            http://ak-outpost-authentik-embedded-outpost.security.svc.cluster.local:9000/outpost.goauthentik.io/auth/nginx
          nginx.ingress.kubernetes.io/auth-signin: |-
            https://vmagent.${SECRET_DOMAIN}/outpost.goauthentik.io/start?rd=$escaped_request_uri
          nginx.ingress.kubernetes.io/auth-response-headers: |-
            Set-Cookie,X-authentik-username,X-authentik-groups,X-authentik-email,X-authentik-name,X-authentik-uid
          nginx.ingress.kubernetes.io/auth-snippet: |
            proxy_set_header X-Forwarded-Host $http_host;
        hosts:
          - &hostagent vmagent.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostagent

    # Single-binary vm cluster alternative (liteweight, non-HA)
    vmsingle:
      enabled: false

    # -- prometheus-node-exporter dependency chart configuration. For possible values check [here](https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-node-exporter/values.yaml)
    prometheus-node-exporter:
      enabled: true

      # all values for prometheus-node-exporter helm chart can be specified here
      service:
        # Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
        #
        labels:
          jobLabel: node-exporter
      extraArgs:
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      # -- Node Exporter VM scrape config
      vmScrape:
        # whether we should create a service scrape resource for node-exporter
        enabled: true

        # -- [Scrape configuration](https://docs.victoriametrics.com/operator/api#vmservicescrapespec) for Node Exporter
        spec:
          jobLabel: jobLabel
          selector:
            matchLabels:
              app.kubernetes.io/name: '{{ include "prometheus-node-exporter.name" (index .Subcharts "prometheus-node-exporter") }}'
          endpoints:
            - port: metrics
              metricRelabelConfigs:
                - action: drop
                  source_labels: [mountpoint]
                  regex: "/var/lib/kubelet/pods.+"

    # -- kube-state-metrics dependency chart configuration. For possible values check [here](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml)
    kube-state-metrics:
      enabled: true
      # -- [Scrape configuration](https://docs.victoriametrics.com/operator/api#vmservicescrapespec) for Kube State Metrics
      vmScrape:
        enabled: true
        spec:
          selector:
            matchLabels:
              app.kubernetes.io/name: '{{ include "kube-state-metrics.name" (index .Subcharts "kube-state-metrics") }}'
              app.kubernetes.io/instance: '{{ include "vm.release" . }}'
          endpoints:
            - port: http
              honorLabels: true
              metricRelabelConfigs:
                - action: labeldrop
                  regex: (uid|container_id|image_id)
          jobLabel: app.kubernetes.io/name

    # -- Component scraping the kubelets
    kubelet:
      enabled: true

    # Component scraping the kube api server
    kubeApiServer:
      # -- Enable Kube Api Server metrics scraping
      enabled: false

    # Add kubeControllerManager scrape
    kubeControllerManager:
      enabled: false
        # - 192.168.108.149
        # - 192.168.108.209
        # - 192.168.108.238

    # Component scraping kubeDns. Use either this or coreDns
    kubeDns:
      # -- Enabled KubeDNS metrics scraping
      enabled: false

    # Component scraping coreDns. Use either this or kubeDns
    coreDns:
      # -- Enabled CoreDNS metrics scraping
      enabled: true
      service:
        # -- Create service for CoreDNS metrics
        enabled: true
        # -- CoreDNS service port
        port: 9153
        # -- CoreDNS service target port
        targetPort: 9153
        # -- CoreDNS service pod selector
        selector:
          k8s-app: kube-dns

      # -- Spec for VMServiceScrape CRD is [here](https://docs.victoriametrics.com/operator/api.html#vmservicescrapespec)
      vmScrape:
        spec:
          jobLabel: jobLabel
          namespaceSelector:
            matchNames: [kube-system]
          endpoints:
            - port: http-metrics
              bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token

    # Add kubeEtcd scrape
    kubeEtcd:
      enabled: false

    # Add kubeScheduler scrape
    kubeScheduler:
      enabled: false

    kubeProxy:
      enabled: false # Disabled because eBPF

    # Enable deployment of grafana
    # I chose to deploy this seperately and manage it myself
    #   insetad of trusting the stack chart
    grafana:
      enabled: false

    ## install vm operator crds
    crds:
      enabled: true

    ## install prometheus operator crds
    prometheus-operator-crds:
      enabled: true