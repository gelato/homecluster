---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-stack
  namespace: observability
spec:
  interval: 30m
  chart:
    spec:
      chart: victoria-metrics-k8s-stack
      version: 0.23.0
      sourceRef:
        kind: HelmRepository
        name: victoriametrics-charts
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    fullnameOverride: victoria-metrics
    argocdReleaseOverride: $ARGOCD_APP_NAME

    # VM Operator deployment
    victoria-metrics-operator:
      enabled: true
      createCRD: false
      operator:
        disable_prometheus_converter: false # Ensure we keep enabled the converter to sync prom rules to VM rules
        enable_converter_ownership: true # Required to allow VM to remove VM rules it imports if a prometheus rule is deleted
      env:
        - name: VM_VMSINGLEDEFAULT_CONFIGRELOADERCPU
          value: 100m
        - name: VM_VMSINGLEDEFAULT_CONFIGRELOADERMEMORY
          value: 100Mi
        - name: VM_VMALERTDEFAULT_CONFIGRELOADERCPU
          value: 100m
        - name: VM_VMALERTDEFAULT_CONFIGRELOADERMEMORY
          value: 100Mi
        - name: VM_VMAGENTDEFAULT_CONFIGRELOADERCPU
          value: 100m
        - name: VM_VMAGENTDEFAULT_CONFIGRELOADERMEMORY
          value: 100Mi
        - name: VM_VMALERTMANAGER_CONFIGRELOADERCPU
          value: 100m
        - name: VM_VMALERTMANAGER_CONFIGRELOADERMEMORY
          value: 100Mi
      resources:
        requests:
          cpu: 20m
          memory: 100Mi
          ephemeral-storage: 1Gi
        limits:
          memory: 100Mi
          ephemeral-storage: 2Gi
    serviceAccount:
      # -- Specifies whether a service account should be created
      create: true
    # Main VM Cluster in HA mode.
     # Main VM Cluster in HA mode.
    vmcluster:
      enabled: true
      spec:
        # Duration to keep metrics
        retentionPeriod: "2" # this is months unless a character prefix is added (1d, 1w, etc)
        extraArgs:
          dedup.minScrapeInterval: 30s
        replicationFactor: 3
        # Storage backend
        vmstorage:
          replicaCount: 3
          # required for HA vmagents
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              cpu: 500m
              memory: 1536Mi
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmstorage
          extraArgs:
            dedup.minScrapeInterval: 30s
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: longhorn-storage
                resources:
                  requests:
                    storage: 20Gi
        # Query backend
        vmselect:
          replicaCount: 2 # HA
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 750m
              memory: 2Gi
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vmselect
          cacheMountPath: /select-cache
          extraArgs:
            dedup.minScrapeInterval: 30s
            search.minStalenessInterval: 5m
            vmalert.proxyURL: http://vmalert.${SECRET_DOMAIN}
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: longhorn-storage
                resources:
                  requests:
                    storage: 2Gi
        # Data ingestion backend
        vminsert:
          replicaCount: 2 # HA
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 256Mi
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: vminsert
          extraArgs:
            maxLabelsPerTimeseries: "90" # More needed if you are scraping a LOT of stuff and having issues

      # VM Alerting (however, this just watches & passes alerts to alertmanager)
    vmalert:
      enabled: true
      spec:
        replicaCount: 2
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 150m
            memory: 256Mi
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmalert
        extraArgs:
          external.url: https://vmalert.${SECRET_DOMAIN}
        notifiers:
          - url: http://vmalertmanager-victoria-metrics.observability.svc.cluster.local:9093
      ingress:
        enabled: true
        ingressClassName: internal
        hosts:
          - &hostalert vmalert.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostalert

     # VM Data scraping
    vmagent:
      enabled: true
      spec:
        replicaCount: 1
        shardCount: 2
        scrapeInterval: 30s
        externalLabels:
          cluster: main
        resources:
          requests:
            cpu: 50m
            memory: 256Mi
          limits:
            cpu: 400m
            memory: 512Mi
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: vmagent
        additionalScrapeConfigs:
          name: vm-additional-scrape-configs
          key: prometheus-additional.yaml
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          nginx.ingress.kubernetes.io/auth-url: |-
            http://ak-outpost-authentik-embedded-outpost.security.svc.cluster.local:9000/outpost.goauthentik.io/auth/nginx
          nginx.ingress.kubernetes.io/auth-signin: |-
            https://vmagent.${SECRET_DOMAIN}/outpost.goauthentik.io/start?rd=$escaped_request_uri
          nginx.ingress.kubernetes.io/auth-response-headers: |-
            Set-Cookie,X-authentik-username,X-authentik-groups,X-authentik-email,X-authentik-name,X-authentik-uid
          nginx.ingress.kubernetes.io/auth-snippet: |
            proxy_set_header X-Forwarded-Host $http_host;
        hosts:
          - &hostagent vmagent.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostagent

    # Single-binary vm cluster alternative (liteweight, non-HA)
    vmsingle:
      enabled: false

    # Extra slack templates
    monzoTemplate:
      enabled: false

    # Prepared sets of default rules
    # Adjust to what scraping functions you have enabled
    # i.e. if you dont have kubeapisever setup & enabled, disable
    #   the kubeApiserver rules below
    defaultRules:
      create: true
      rules:
        etcd: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverBurnrate: true
        kubeApiserverHistogram: true
        kubeApiserverSlos: true
        kubelet: true
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        vmagent: true
        vmsingle: true
        vmhealth: true
        alertmanager: true

      # -- Runbook url prefix for default rules
      runbookUrl: https://runbooks.${SECRET_DOMAIN}

      ## Reduce app namespace alert scope
      appNamespacesTarget: ".*"

    ## -- Create default dashboards
    defaultDashboardsEnabled: true

    ## -- Create experimental dashboards
    experimentalDashboardsEnabled: true

    # Prometheus alertmanager spinup
    # Wait, why are we using vmalert AND prometheus alertmanager???
    # VMalert isnt fully featured and VM acknowledge its a favourite
    # for example,
    alertmanager:
      enabled: true

      ingress:
        enabled: true
        ingressClassName: internal
        hosts:
          - &hostam alertmanager.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *hostam
      spec:
        externalURL: https://alertmanager.${SECRET_DOMAIN}
        replicaCount: 1
        # Import Alertmanager config from a existing secret
        # this can be configured directly in chart but it can
        #   be a bit unweildy to manage
        configSecret: alertmanager-secret
        # I needed this for talos using local-hostpath - YMMV
        #
        securityContext:
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          fsGroup: 2000
          seccompProfile:
            type: RuntimeDefault

        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-storage
              resources:
                requests:
                  storage: 50Mi

    # prometheus-node-exporter dependency chart configuration. For possible values refer to https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-node-exporter/values.yaml
    prometheus-node-exporter:
      enabled: true
      prometheus:
        monitor:
          relabelings:
            - action: replace
              targetLabel: "instance"
              sourceLabels:
                - "__meta_kubernetes_pod_node_name"

      ## all values for prometheus-node-exporter helm chart can be specified here
      podLabels:
        ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
        ##
        jobLabel: node-exporter
      extraArgs:
        - "--collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)"
        - "--collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$"

      vmServiceScrape:
        # wheter we should create a service scrape resource for node-exporter
        enabled: true

        # spec for VMServiceScrape crd
        # https://docs.victoriametrics.com/operator/api.html#vmservicescrapespec
        spec:
          jobLabel: app.kubernetes.io/instance
          endpoints:
            - port: metrics
              metricRelabelConfigs:
                - action: drop
                  source_labels: [mountpoint]
                  regex: "/var/lib/kubelet/pods.+"


    # Scrape configs
    kubelet:
      enabled: true
      spec:
        interval: 30s
        serviceMonitor:
          cAdvisorRelabelings:
            - action: replace
              sourceLabels: [__metrics_path__]
              targetLabel: metrics_path
            - action: replace
              targetLabel: instance
              sourceLabels:
                - "node"
          relabelings:
            - action: replace
              sourceLabels: [__metrics_path__]
              targetLabel: metrics_path
          metricRelabelings:
            - action: drop
              regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
              sourceLabels: [__name__]
            - action: drop
              regex: "etcd_request_duration_seconds_bucket"
              sourceLabels: [__name__]
            - action: drop
              regex: (scheduler_plugin_execution_duration_seconds_bucket)
              sourceLabels: [__name__]
            - action: drop
              regex: (workqueue_work_duration_seconds_bucket)
              sourceLabels: [__name__]
        # drop high cardinality label and useless metrics for cadvisor and kubelet
        metricRelabelConfigs:
          - action: labeldrop
            regex: (uid|pod_uid|id)
          - action: labeldrop
            regex: (name)
          - action: drop
            source_labels: [__name__]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)
          - action: drop
            source_labels: [__name__]
            regex: (container_tasks_state|container_memory_failures_total)
          - action: drop
            source_labels: [__name__]
            regex: (container_blkio_device_usage_total)
          - action: drop
            source_labels: [__name__]
            regex: (prober_probe_duration_seconds_bucket)
        relabelConfigs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - sourceLabels: [__metrics_path__]
            targetLabel: metrics_path
          - targetLabel: "job"
            replacement: "kubelet"


    # kube-state-metrics dependency chart configuration. For possible values refer to https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml
    kube-state-metrics:
      enabled: true
      prometheus:
        monitor:
          relabelings:
            - action: replace
              targetLabel: "instance"
              sourceLabels:
                - "__meta_kubernetes_pod_node_name"

    # Add Kubeapi scrape
    kubeApiServer:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: drop
            regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: "etcd_request_duration_seconds_bucket"
            sourceLabels: [__name__]
          - action: drop
            regex: (scheduler_plugin_execution_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: (workqueue_work_duration_seconds_bucket)
            sourceLabels: [__name__]

    # Add kubeControllerManager scrape
    kubeControllerManager:
      enabled: true
      endpoints: &cp
        - 192.168.108.149
        - 192.168.108.238
        - 192.168.108.209
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
        metricRelabelings:
          - action: drop
            regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: "etcd_request_duration_seconds_bucket"
            sourceLabels: [__name__]
          - action: drop
            regex: (scheduler_plugin_execution_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: (workqueue_work_duration_seconds_bucket)
            sourceLabels: [__name__]

    # -Component scraping kubeDns. Use either this or coreDns
    kubeDns:
      enabled: false

    # -- Component scraping coreDns. Use either this or kubeDns
    coreDns:
      spec:
        jobLabel: "jobLabel"
      enabled: true
      service:
        enabled: true
        port: 9153
        targetPort: 9153
        selector:
          k8s-app: kube-dns
      serviceMonitor:
        metricRelabelings:
          - action: drop
            regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: "etcd_request_duration_seconds_bucket"
            sourceLabels: [__name__]
          - action: drop
            regex: (scheduler_plugin_execution_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: (workqueue_work_duration_seconds_bucket)
            sourceLabels: [__name__]

    # Add kubeScheduler scrape
    kubeScheduler:
      enabled: true
      endpoints: *cp
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
        metricRelabelings:
          - action: drop
            regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: "etcd_request_duration_seconds_bucket"
            sourceLabels: [__name__]
          - action: drop
            regex: (scheduler_plugin_execution_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: (workqueue_work_duration_seconds_bucket)
            sourceLabels: [__name__]

    # Add kubeEtcd scrape
    kubeEtcd:
      enabled: true
      endpoints: *cp
      service:
        enabled: true
        port: 2381
        targetPort: 2381
      serviceMonitor:
        metricRelabelings:
          - action: drop
            regex: (apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket|apiserver_request_sli_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: "etcd_request_duration_seconds_bucket"
            sourceLabels: [__name__]
          - action: drop
            regex: (scheduler_plugin_execution_duration_seconds_bucket)
            sourceLabels: [__name__]
          - action: drop
            regex: (workqueue_work_duration_seconds_bucket)
            sourceLabels: [__name__]

    grafana:
      enabled: false

    ## install vm operator crds
    crds:
      enabled: false